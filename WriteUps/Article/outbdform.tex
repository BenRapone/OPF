\section{Computing Upper Bounds on the Robustness Margin} \label{sec:outbdform}  

Due to the hardness of solving the problem $~\displaystyle \min\limits_{\vx\in \partial \Omega, \vu\in\Omega_u}\max\limits_{\norm{\vlambda}=1 }\ \vlambda^T\left(F(\vx)-\vu\right),\,$ we derived a sequence of relaxations in order to arrive at a method that was computationally efficient for non-trivial data sets. 
We can similarly work with $~\displaystyle \min\limits_{\norm{\vlambda}=1}\max\limits_{\vx\in \bar{\Omega}, \vu\in\Omega_u}\ \vlambda^T\left(F(\vx)-\vu\right) \,$ to produce computationally efficient procedures to compute upper bounds for the robustness margin. 

\bigskip
\begin{thm}\label{thm:OPTfeasOut} 
Let $\bar{\Omega}=\{\vx| A\vx\leq \vb\}$, $\Omega_{u}=\{\vu| u^{\min}_i\leq u_i \leq u^{\max}_i \ \forall i \}$, and $F(\vx)=Q(\vx)+L\vx$ as described in \cref{eq:Quad,eq:xLimits,eq:uLimits}. 
Let
\[
  z = \min\limits_{\norm{\vlambda}=1}\max\limits_{\vx\in \bar{\Omega}, \vu\in\Omega_u}\ \vlambda^T\left(F(\vx)-\vu\right).
\]
If there is an $r>0$ such that $r \leq e_i \ \mbox{ for all } i \mbox{ with } e_i>0$, where $e_i$ is the error bound associated with $ u^{\min}_i$ and $ u^{\max}_i$, and if $z=0$ then the system has robustness margin of no more than r.

\medskip
\begin{proof} 
  Observe by \cref{lem:BdOpt} that if $\min\limits_{\norm{\vlambda}=1}\max\limits_{\vx\in \bar{\Omega}, \vu \in \Omega_u}\ \vlambda^T\left(F(\vx)-\vu\right)=0$, then the statement in \cref{eq:RSForm} is invalidated.  
%
%Observe that if \cref{eq:RSForm} is invalidated then $\exists \hat{\vx}\in\partial\Omega$ such that $F(\hat{\vx})=\hat{u}$ for some $\hat{u}\in\Omega_{u}$ and thus certainly $$\min\limits_{\norm{\vlambda}=1}\max\limits_{x\in X, u\in\Omega_u}\ \vlambda^T\left(F(x)-u\right) \geq \min\limits_{\norm{\vlambda}=1}\ \vlambda^T\left(F(\hat{\vx})-\hat{u}\right)=0.$$ The theorem now follows.
%\end{itemize}
\end{proof}
\end{thm}

\bigskip
We can relax the computation suggested in \cref{thm:OPTfeasOut} by utilizing the same techniques as before, replacing $\vx\vx^T$ with a positive semidefinite matrix $X$, with the option of dropping the condition that $X$ be positive semidefinite.
This step transforms the computation from a semidefinite program to a linear or mixed integer program depending on how one deals with the constraint $\norm{\vlambda}=1$. 
In our tests we use the $\ell_1$ norm and introduce variables that capture the absolute value of each $\lambda_i$.
We obtain the following procedure.
%In our tests we iterate over all possible sign choices for each dimension of $\vlambda$ as there are only ten dimensions of variability in our applications. \\

\bigskip
\bigskip
\textit{Outer Bound Procedure}: Formulation
\begin{equation}\label{eq:OPTfeasOutRelaxa}
\begin{array}{rl}
 \ z &=\min\limits_{\norm{\vlambda}=1}\max\limits_{\vx} \, \vlambda^T\left(F(\vx)-\vu^* \right) \\
 \text{subject to } \ & A\vx\leq \vb \\
 	&\vb\vb^T-A\vx\vb^T-\vb(A\vx)^T+AXA^T\geq O \\
 	&X \text{ is symmetric.}
\end{array}
\end{equation}

\bigskip


%We can construct the dual of the inner maximal objective of \cref{eq:OPTfeasOutRelaxa} as follows: \\

%\textbf{Outer Bound Procedure B} 
%\begin{equation}\label{eq:OPTfeasOutRelaxb}
%\begin{array}{rl}
%\ z &=\min\limits_{\norm{\vlambda}=1,\vy}B^T\vy  \\
% \text{subject to: } \ & H^T\vy=\vu(\lambda) \\
% & \vy\leq 0
%\end{array}
%\end{equation}
%where the constraints of \cref{eq:OPTfeasOutRelaxa} can be written as $H\vx\geq B$ and the objective of \cref{eq:OPTfeasOutRelaxa} as $\vu(\lambda)^T\vx$. 
%Notice that the objective of \cref{eq:OPTfeasOutRelaxa} is different from that of \cref{thm:OPTfeasOut}. 
%The optimal value of the objectives of  \cref{eq:OPTfeasOutRelaxa} and \cref{eq:OPTfeasOutRelaxb} are much easier to find and they allow us to directly solve for the minimal outer bound approximation of the robustness margin. 

%Given the initial equation $$Q(\vx)+L\vx-\vu^* = \textbf{0}$$ with the constraint that $A\vx\leq \vb$ we wish to discover upper bounds on the robust margin of $\vu^*$. We define $Q(\vx)$ to be $[\vx^TQ_1\vx,\vx^TQ_2\vx,...,\vx^TQ_n\vx]^T$ relaxed to $[Tr\left(Q_1X\right),Tr\left(Q_2X\right),...,Tr\left(Q_nX\right)]^T$ where $X$ is a symmetric matrix. 
%\begin{align*}
%\min\limits_{||\lambda||=1}\max\limits_{\vx} & \lambda^T\left[Tr\left(QX\right)+L\vx-\vu^*\right]
%\intertext{\hspace*{0.75in}\textbf{Subject To:}} 
%(1) \ &A\vx\leq \vb \\
%(2) \ &\vb\vb^T-A\vx\vb^T-\vb(A\vx)^T+AXA^T\geq 0 \\
%(3) \ &X \text{ is symmetric, positive semidefinite}
%\end{align*}

In order to implement this procedure, we use linear programming duality to write the problem in \cref{eq:OPTfeasOutRelaxa} as direct minimization LP (in place of a min-max problem).
To construct the dual of the inner maximum objective function, we first write the constraints as $M\hat{\vx} \leq \vB$ and the objective as $g(Q,L,\lambda)\hat{\vx}-\lambda^T\vu^*$, where $\hat{\vx}^T=[\vx^T ~\hat{X}^T]$, with $\hat{X}$ being the vector form of the upper triangular (including diagonal) portion of $X$. 
We enforce the constraint that $X$ is symmetric by utilizing only the upper triangular portion of $X$.
We can write $M \hat{\vx} \leq \vB$ using only the upper triangular entries including the diagonal as follows.

\medskip
%\noindent
%\scalebox{0.8}{
\[
\resizebox{\hsize}{!}
{
  \begin{bmatrix}
    A_{11} & \dots & A_{1n} & 0 & \dots & 0 \\ 
    %	A_{21} & A_{22} & \dots & A_{2n} & 0 & 0 & \dots & 0 \\  
    \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
    A_{m1} & \dots & A_{mn} & 0 & \dots & 0 \\
    \vb_1A_{11}+\vb_1A_{11} & \dots & \vb_1A_{1n}+\vb_1A_{1n} & -A_{11}A_{11} & \dots & -A_{1n}A_{1n} \\ 
    %\vb_2A_{11}+\vb_1A_{21} & \vb_2A_{12}+\vb_1A_{22} & \dots & \vb_2A_{1n}+\vb_1A_{2n} & -A_{11}A_{21} & -A_{12}A_{21}-A_{11}A_{22} & \dots & -A_{1n}A_{2n} \\ 
    \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
    \vb_rA_{q1}+\vb_qA_{r1} & \dots & \vb_rA_{qn}+\vb_qA_{rn} & -A_{q1}A_{r1} & \dots & -A_{1n}A_{2n} \\ 
    \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
    \vb_mA_{m1}+\vb_mA_{m1} & \dots & \vb_mA_{mn}+\vb_mA_{mn} & -A_{m1}A_{m1} & \dots & -A_{mn}A_{mn} 
  \end{bmatrix}

  \begin{bmatrix}
    x_1 \\ 
    %\vx_2 \\  
    \vdots \\
    x_n \\
    X_{11} \\ 
    %X_{12}  \\
    \vdots \\
    X_{qr}\\
    %X_{23}\\
    \vdots \\
    X_{nn} 
  \end{bmatrix}%

  ~\leq~

  \begin{bmatrix}
    b_1 \\ 
    %b_2 \\  
    \vdots \\
    b_m \\
    b_1b_1 \\ 
    %b_1b_2  \\
    \vdots \\
    b_qb_r\\
    %b_2b_3\\
    \vdots \\
    b_mb_m 
  \end{bmatrix}.%
}
\]
%\bigskip
%
We can write the objective function as $g(Q,L,\lambda)\hat{\vx}-\lambda^T\vu^*$ in the following way.
%
%\bigskip
\[
\resizebox{\hsize}{!}
{
  \begin{bmatrix}
    \sum\limits_{j=1}^nL_{j,1}\lambda_j & %\sum\limits_{j=1}^nL_{j,2}\lambda_j &
    \dots &
    \sum\limits_{j=1}^nL_{j,n}\lambda_j &
    \sum\limits_{j=1}^n\lambda_jQ_{11}^j & % \sum\limits_{j=1}^n\lambda_j(Q_{12}^j+Q_{21}^j) &
    \dots &
    \sum\limits_{j=1}^n\lambda_jQ_{nn}^j 
  \end{bmatrix}
  
  \begin{bmatrix}
    x_1 \\ 
    %x_2 \\  
    \vdots \\
    x_n \\
    X_{11} \\ 
    %X_{12}  \\
    \vdots \\
    %X_{22}\\
    %X_{23}\\
    %\vdots \\
    X_{nn} 
  \end{bmatrix}%
  --
  \begin{bmatrix}
    \lambda_1 &  \dots & \lambda_n 
  \end{bmatrix}

  \begin{bmatrix}
    u^*_1 \\ 
    %u^*_2 \\  
    \vdots \\
    u^*_n \\
  \end{bmatrix}.%
}
\]
If $\vu^*=\vzero$, then the objective function reduces to just $g(Q,L,\lambda)\hat{\vx}$.

\smallskip
On the other hand, if $\vu^*\neq \vzero$ then we can add an extra dummy variable $\vx_{n+1}$ with the constraint $\vx_{n+1}=1$, to obtain the following system.

\[
\resizebox{\hsize}{!}
{
  \begin{bmatrix}
    A_{11} & \dots & A_{1n} & A_{1,n+1} & 0 & \dots & 0 \\ 
    %A_{21} & \dots & A_{2n} & A_{2,n+1} & 0 & \dots & 0 \\  
    \vdots & \ddots & \vdots & \p \vdots & \vdots & \ddots & \vdots\\
    A_{m1} & \dots & A_{mn} & A_{mn+1} & 0 & \dots & 0 \\
    0 & \dots & 0 & \p 1 & 0 & \dots & 0 \\
    0 & \dots & 0 & -1 & 0 & \dots & 0 \\
    \vb_1A_{11}+\vb_1A_{11} & \dots & \vb_1A_{1n}+\vb_1A_{1n} & \p 0 & -A_{11}A_{11} & \dots & -A_{1n}A_{1n} \\ 
    %\vb_2A_{11}+\vb_1A_{21} & \dots & \vb_2A_{1n}+\vb_1A_{2n} &  0 & -A_{11}A_{21} & \dots & -A_{1n}A_{2n} \\ 
    \vdots & \ddots & \vdots & \p \vdots & \vdots & \ddots & \vdots\\
    \vb_rA_{q1}+\vb_qA_{r1} & \dots & \vb_rA_{qn}+\vb_qA_{rn} & \p 0 & -A_{q1}A_{r1} & \dots & -A_{1n}A_{2n} \\ 
    \vdots & \ddots & \vdots & \p \vdots & \vdots & \ddots & \vdots\\
    \vb_mA_{m1}+\vb_mA_{m1} & \dots & \vb_mA_{mn}+\vb_mA_{mn} & \p 0 & -A_{m1}A_{m1} & \dots & -A_{mn}A_{mn} 
  \end{bmatrix}%

  \begin{bmatrix}
    x_1 \\ 
    %x_2 \\  
    \vdots \\
    x_n \\
    x_{n+1} \\
    X_{11} \\ 
    %X_{12}  \\
    \vdots \\
    X_{qr}\\
    %X_{23}\\
    \vdots \\
    X_{nn} 
  \end{bmatrix}%

  ~\leq~
  
  \begin{bmatrix}
    \p b_1 \\ 
    %b_2 \\  
    \p \vdots \\
    \p b_m \\
    \p 1 \\
    -1 \\
    \p b_1b_1 \\ 
    %b_1b_2  \\
    %\p \vdots \\
    %b_2b_2\\
    %b_2b_3\\
    \p \vdots \\
    b_qb_r\\
    \p \vdots \\
    b_mb_m 
  \end{bmatrix}.%
}
\]
%
We can then write the objective function $g(Q,L,\lambda)\hat{\vx}$ as follows.
%
\[
\resizebox{0.9\hsize}{!}
{
  \begin{bmatrix}
    \sum\limits_{j=1}^nL_{j,1}\lambda_j &
    \dots &
    \sum\limits_{j=1}^nL_{j,n}\lambda_j &
    \sum\limits_{j=1}^n-\lambda_j u^*_j &
    \sum\limits_{j=1}^n\lambda_jQ_{11}^j &
    %\sum\limits_{j=1}^n\lambda_j(Q_{12}^j+Q_{21}^j) &
    \dots &
    \sum\limits_{j=1}^n\lambda_jQ_{nn}^j 
  \end{bmatrix}

  \begin{bmatrix}
	x_1 \\ 
	%x_2 \\  
	\vdots \\
	x_n \\
	x_{n+1} \\
	X_{11} \\ 
	%X_{12}  \\
	\vdots \\
 	%X_{22}\\
	%X_{23}\\
	\vdots \\
	X_{nn} 
  \end{bmatrix}.%
}
\] 

In either case, the final optimization problem, and hence the procedure we use in our tests, is given as follows.

\medskip
\textbf{Outer Bound Procedure} 
\begin{equation}\label{eq:OPTfeasOutRelaxb}
\begin{array}{rl}
  \ z &=\min\limits_{\norm{\vlambda}=1, \vy}\vB^T\vy  \\
  \vspace*{-0.15in} \\
 \text{subject to } \ & M^T\vy = g(Q,L,\lambda)^T \\
 & \vy \geq \vzero .
\end{array}
\end{equation}

%$$\min\limits_{||\lambda||=1, \vy}\vB^T\vy$$
%\begin{align*}
%\intertext{\textbf{Subject To:}}
%(1) \ & M^T\vy \geq g(Q,L,\lambda)^T \\ 
%(2) \ & \vy\geq 0\\
%\end{align*}









