
As a result of the complexity of $\min\limits_{\bold x\in \partial \Omega, \bold u\in\Omega_u}\max\limits_{||\bold \lambda||=1 }\ \bold\lambda^T\left(F(\bold x)-\bold u\right)$ we derived a sequence of relaxations in order to arrive at a method that was computationally efficient for a non-trivial data set. 
We can similarly work with $\min\limits_{||\bold{\lambda}||=1}\max\limits_{\bold x\in \bar{\Omega}, \bold u\in\Omega_u}\ \bold{\lambda}^T\left(F(\bold x)-\bold u\right)$ to produce computationally efficient models for outer bound approximations of the robustness margin. \\
\begin{thm}\label{thm:OPTfeasOut} 
Let $\bar{\Omega}=\{\bold x| A\bold x\leq \bold b\}$, $\Omega_{u}=\{\bold u| u^{\min}_i\leq u_i \leq u^{\max}_i \ \forall i \}$, and $F(\bold x)=Q(\bold x)+L(\bold x)$ as described in \eqref{eq:Quad}, \eqref{eq:xLimits} and \eqref{eq:uLimits}. 
Let
$$z = \min\limits_{||\bold{\lambda}||=1}\max\limits_{\bold x\in \bar{\Omega}, \bold u\in\Omega_u}\ \bold{\lambda}^T\left(F(\bold x)-\bold u\right).$$
If $r>0$ s.t. $r\leq e_i \ \forall i \ s.t. \ e_i>0$ where $e$ denotes the error bounds associated with $ u^{\min}_i$ and $ u^{\max}_i$, and if $z=0$ then the system has robustness margin of no more than r.

\begin{proof} \ \\
Observe by \cref{lem:BdOpt} that if $\min\limits_{||\bold{\lambda}||=1}\max\limits_{\bold x\in \Omega, \bold u\in\Omega_u}\ \bold{\lambda}^T\left(F(\bold x)-\bold u\right)=0$, then \eqref{eq:RSForm} is invalidated.  

%Observe that if \eqref{eq:RSForm} is invalidated then $\exists \hat{x}\in\partial\Omega$ s.t. $F(\hat{x})=\hat{u}$ for some $\hat{u}\in\Omega_{u}$ and thus certainly $$\min\limits_{||\bold{\lambda}||=1}\max\limits_{x\in X, u\in\Omega_u}\ \bold{\lambda}^T\left(F(x)-u\right) \geq \min\limits_{||\bold{\lambda}||=1}\ \bold{\lambda}^T\left(F(\hat{x})-\hat{u}\right)=0.$$ The theorem now follows.
\end{proof}
\end{thm}

We can relax the model in \cref{thm:OPTfeasOut} utilizing the same techniques as before, replacing $\bold x\bold x^T$ with a positive semidefinite matrix $X$, with the option of dropping the condition that $X$ be positive semidefinite, which transforms the model from a semidefinite program to a linear or mixed integer program depending on how one deals with the condition $||\bold{\lambda}||=1$. 
In our tests we iterate over all possible sign choices for each dimension of $\bold \lambda$ as there are only ten dimensions of variability in our applications. 


\begin{subequations}\label{eq:OPTfeasOutRelaxa}
\begin{align}
\text{Outer Bound Model A}& \ z=\min\limits_{||\bold{\lambda}||=1}\max\limits_{\bold x}  \bold{\lambda}^T\left(F(\bold x)-\bold u^*\right)\\
 \text{subject to: } \ & A\bold x\leq \bold b \\
 	&\bold b\bold b^T-A\bold x\bold b^T-\bold b(A\bold x)^T+A\mathcal{X}A^T\geq O \\
 	&\mathcal{X} \text{ is symmtric}
\end{align}
\end{subequations}

We can construct the dual of the inner maximal objective of \eqref{eq:OPTfeasOutRelaxa} to produce:

\begin{subequations}\label{eq:OPTfeasOutRelaxb}
\begin{align}
\text{Outer Bound Model B}& \ z=\min\limits_{||\bold{\lambda}||=1,\bold y}B^T\bold y  \\
 \text{subject to: } \ & H^T\bold y=\bold u(\lambda) \\
 & \bold y\leq 0
\end{align}
where \eqref{eq:OPTfeasOutRelaxa}$[b \& c]$ can be written as $H\bold x\geq B$ and the objective of \eqref{eq:OPTfeasOutRelaxa} as $\bold u(\lambda)^T\bold x$. 
Notice that the objective of \eqref{eq:OPTfeasOutRelaxa} is different from that of \cref{thm:OPTfeasOut}. 
The optimal value of the objectives of  \eqref{eq:OPTfeasOutRelaxa} and \eqref{eq:OPTfeasOutRelaxb} are much easier to find and they allow us to directly solve for the minimal outer bound approximation of the robustness margin. 
\end{subequations}
